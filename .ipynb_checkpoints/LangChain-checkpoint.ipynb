{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa31dfc-b572-4850-9a77-f7340da4245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b49aba-803a-43da-9f11-fe00753f09bc",
   "metadata": {},
   "source": [
    "\n",
    "# 강의1\n",
    "- 링크 : https://www.youtube.com/watch?v=EWKbZFqiCsE&t=432s\n",
    "- Langchain : llm 개발을 위한 도구 모음\n",
    "- 필요한 이유 : 따로따로 붙일 수 있지만 통합 도구가 있어서 편리할 때 사용 가능\n",
    " > part끼리 연결, 교체 편함 ->(내 생각) 유지보수가 쉬울 것 같음\n",
    " > 추상화로 코드 단순화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54d6545-32f5-4e92-aec1-28b8f1f10418",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp38-cp38-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain) (3.10.11)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from openai) (4.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp38-cp38-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.43->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp38-cp38-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pkrng\\desktop\\재취업준비\\공부\\torch_test\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Downloading langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.78.1-py3-none-any.whl (680 kB)\n",
      "   ---------------------------------------- 0.0/680.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 680.9/680.9 kB 26.2 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp38-cp38-win_amd64.whl (198 kB)\n",
      "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 29.4 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading greenlet-3.1.1-cp38-cp38-win_amd64.whl (298 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: tenacity, jsonpatch, jiter, greenlet, distro, async-timeout, SQLAlchemy, requests-toolbelt, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.40 async-timeout-4.0.3 distro-1.9.0 greenlet-3.1.1 jiter-0.9.0 jsonpatch-1.33 langchain-0.2.17 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 openai-1.78.1 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d36e4-b6ec-472d-9861-4d3528ff5e23",
   "metadata": {},
   "source": [
    "# 강의2\n",
    "- Llama-3-Open-Ko-8B-Instruct-preview를 이용한 한국어 LLM 튜토리얼\n",
    " > [유튜브 빵형의 개발도상국](https://www.youtube.com/@bbanghyong)\n",
    " > https://huggingface.co/beomi/Llama-3-Open-Ko-8B-Instruct-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973e79f6-6d72-4773-a829-5e6ee818ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install transformers==4.40.0 accelerate gradio \n",
    "# # -q 적은 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a317e9-44f4-4cc3-8c44-bc434a5a362d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decaf2628edf43ee9753915ae8266038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'beomi/Llama-3-Open-Ko-8B-Instruct-preview'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dabcd4-6d9e-43b6-9b16-ffb624fe191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "PROMPT = '''당신은 논문을 한국어로 요약해주는 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'''\n",
    "instruction = \"다음 제목의 논문을 요약해줘 'Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean'\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df70a35-42bd-4b7b-b086-cbfb28ca5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8021bc-f704-4396-a042-c9a22a6424ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# # PROMPT = '''당신은 20대 초반의 여성이고 이름은 '빵순이'입니다. 귀여운 말투로 메신저로 대화하듯이 간결하고 정확하게 대답해야 합니다.'''\n",
    "# PROMPT = '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'''\n",
    "\n",
    "# def generate_response(input_text, chat_history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}]\n",
    "\n",
    "#     for message in chat_history:\n",
    "#         messages.append({\"role\": \"user\", \"content\": message[0]})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": message[1]})\n",
    "\n",
    "#     messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     terminators = [\n",
    "#         tokenizer.eos_token_id,\n",
    "#         tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "#     ]\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         temperature=1, #0 딱딱한 답변, 1은 유연하게\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "\n",
    "#     response = outputs[0][input_ids.shape[-1]:]\n",
    "#     response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "#     chat_history.append((input_text, response))\n",
    "\n",
    "#     return \"\", chat_history\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     chatbot = gr.Chatbot(height=600)\n",
    "#     msg = gr.Textbox()\n",
    "#     clear = gr.Button(\"Clear\")\n",
    "\n",
    "#     msg.submit(generate_response, [msg, chatbot], [msg, chatbot])\n",
    "#     clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b64b8c8-1646-4695-8714-8305276a48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchian_sample1.py \n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/{name}\")\n",
    "#async def root():\n",
    "def root(name:str):\n",
    "    return {\"message\": \"not home_\"+name}\n",
    "\n",
    "\n",
    "@app.get(\"/home\")\n",
    "def home(name:str):\n",
    "    return {\"message\": \"home\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c336b7-01d2-436e-849e-278e7203fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c71a517-390f-4d00-bc5c-bbd5937a2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Hello World'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235330af-be45-4867-a5c3-0c6d5682a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_llm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_llm.py \n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "app = FastAPI()\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 100\n",
    "def llm(x):\n",
    "    return x\n",
    "@app.get(\"/generate/{prompt}\")\n",
    "#async def root():\n",
    "def root(prompt:str): #차후 이걸로 대체 req:PromptRequest\n",
    "    result = llm(prompt)\n",
    "    return {\"message\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea7a9d-4515-4f08-ad8f-41fc175e8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "app = FastAPI()\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 100\n",
    "\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Q: {question}\\nA:\"\n",
    ")\n",
    "\n",
    "@app.get(\"/ask\")\n",
    "def ask(question: str):\n",
    "    prompt = template.format(question=question)\n",
    "    response = llm(prompt)\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f49757-fa43-469c-90af-1168a836e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 터미널 명령어 \n",
    "! uvicorn sample1:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f02556-fc60-4a9f-8105-cd9f2caaf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://127.0.0.1:8000/docs #Swagger UI - 설계, 빌드, 문서화, 소비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb27dea-a90e-47a8-adec-3f60e56ff0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uvicorn sample2:app --reload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_test",
   "language": "python",
   "name": "torch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
